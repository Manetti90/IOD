{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b",
   "metadata": {
    "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612",
   "metadata": {
    "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612"
   },
   "source": [
    "# Lab 8.5 - Prompting Large Language Models\n",
    "\n",
    "In this lab we will practise prompting with a few Large Language Models (LLMs) using Groq (not to be confused with Grok). Groq is a platform that provides access to their custom-built AI hardware via APIs, allowing users to run open-source models such as Llama.\n",
    "\n",
    "We shall see that while LLMs are powerful tools, how you ask a question or frame a task can dramatically influence the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a",
   "metadata": {
    "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719",
   "metadata": {
    "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719"
   },
   "source": [
    "Step 1: Sign up for a free Groq account at https://console.groq.com/home .\n",
    "\n",
    "Step 2: Create a new API key at https://console.groq.com/keys. Copy-paste it into an empty text file called 'groq_key.txt'.\n",
    "\n",
    "Running the next cell will then read in this key and assign it to the variable `groq_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56441b82-ddc5-46e9-9583-878e8a807b7f",
   "metadata": {
    "id": "56441b82-ddc5-46e9-9583-878e8a807b7f"
   },
   "outputs": [],
   "source": [
    "groqfilename = r'groq_key.txt' # this file contains a single line containing your Groq API key only\n",
    "try:\n",
    "    with open(groqfilename, 'r') as f:\n",
    "        groq_key = f.read().strip()\n",
    "except FileNotFoundError:\n",
    "    print(\"'%s' file not found\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88",
   "metadata": {
    "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Using cached groq-0.28.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\matth\\anaconda3\\envs\\cohort2\\lib\\site-packages (from groq) (4.6.2)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\matth\\anaconda3\\envs\\cohort2\\lib\\site-packages (from groq) (0.27.0)\n",
      "Collecting pydantic<3,>=1.9.0 (from groq)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\matth\\anaconda3\\envs\\cohort2\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\matth\\anaconda3\\envs\\cohort2\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\matth\\anaconda3\\envs\\cohort2\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\matth\\anaconda3\\envs\\cohort2\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\matth\\anaconda3\\envs\\cohort2\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\matth\\anaconda3\\envs\\cohort2\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Using cached groq-0.28.0-py3-none-any.whl (130 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, distro, annotated-types, pydantic, groq\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 groq-0.28.0 pydantic-2.11.7 pydantic-core-2.33.2 typing-inspection-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c",
   "metadata": {
    "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from groq import Groq\n",
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227",
   "metadata": {
    "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227"
   },
   "source": [
    "First create an instance of the Groq client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b",
   "metadata": {
    "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b"
   },
   "outputs": [],
   "source": [
    "client = Groq(api_key=groq_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097cd77-7a8d-4502-ae6e-607782706b26",
   "metadata": {
    "id": "a097cd77-7a8d-4502-ae6e-607782706b26"
   },
   "source": [
    "The following code shows what models are currently accessible through Groq. `context_window` refers to the size of memory (in tokens) during a session and `max_completion_tokens` is the maximum number of tokens that are generated in an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4",
   "metadata": {
    "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>active</th>\n",
       "      <th>context_window</th>\n",
       "      <th>public_apps</th>\n",
       "      <th>max_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-86m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632165</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-22m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632101</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>qwen/qwen3-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1748396646</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>40960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meta-llama/llama-guard-4-12b</td>\n",
       "      <td>model</td>\n",
       "      <td>1746743847</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meta-llama/llama-4-maverick-17b-128e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743877158</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>meta-llama/llama-4-scout-17b-16e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743874824</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>compound-beta-mini</td>\n",
       "      <td>model</td>\n",
       "      <td>1742953279</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>qwen-qwq-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1741214760</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>compound-beta</td>\n",
       "      <td>model</td>\n",
       "      <td>1740880017</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>playai-tts-arabic</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682783</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>playai-tts</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682771</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mistral-saba-24b</td>\n",
       "      <td>model</td>\n",
       "      <td>1739996492</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>True</td>\n",
       "      <td>32768</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737924940</td>\n",
       "      <td>DeepSeek / Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>allam-2-7b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737672203</td>\n",
       "      <td>SDAIA</td>\n",
       "      <td>True</td>\n",
       "      <td>4096</td>\n",
       "      <td>None</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>model</td>\n",
       "      <td>1733447754</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>whisper-large-v3-turbo</td>\n",
       "      <td>model</td>\n",
       "      <td>1728413088</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3-8b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>distil-whisper-large-v3-en</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Hugging Face</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama3-70b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>whisper-large-v3</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id object     created  \\\n",
       "17            meta-llama/llama-prompt-guard-2-86m  model  1748632165   \n",
       "0             meta-llama/llama-prompt-guard-2-22m  model  1748632101   \n",
       "16                                 qwen/qwen3-32b  model  1748396646   \n",
       "3                    meta-llama/llama-guard-4-12b  model  1746743847   \n",
       "10  meta-llama/llama-4-maverick-17b-128e-instruct  model  1743877158   \n",
       "15      meta-llama/llama-4-scout-17b-16e-instruct  model  1743874824   \n",
       "7                              compound-beta-mini  model  1742953279   \n",
       "14                                   qwen-qwq-32b  model  1741214760   \n",
       "20                                  compound-beta  model  1740880017   \n",
       "4                               playai-tts-arabic  model  1740682783   \n",
       "5                                      playai-tts  model  1740682771   \n",
       "11                               mistral-saba-24b  model  1739996492   \n",
       "19                  deepseek-r1-distill-llama-70b  model  1737924940   \n",
       "13                                     allam-2-7b  model  1737672203   \n",
       "6                         llama-3.3-70b-versatile  model  1733447754   \n",
       "21                         whisper-large-v3-turbo  model  1728413088   \n",
       "2                                  llama3-8b-8192  model  1693721698   \n",
       "1                                    gemma2-9b-it  model  1693721698   \n",
       "12                     distil-whisper-large-v3-en  model  1693721698   \n",
       "9                                 llama3-70b-8192  model  1693721698   \n",
       "8                                whisper-large-v3  model  1693721698   \n",
       "18                           llama-3.1-8b-instant  model  1693721698   \n",
       "\n",
       "           owned_by  active  context_window public_apps  max_completion_tokens  \n",
       "17             Meta    True             512        None                    512  \n",
       "0              Meta    True             512        None                    512  \n",
       "16    Alibaba Cloud    True          131072        None                  40960  \n",
       "3              Meta    True          131072        None                   1024  \n",
       "10             Meta    True          131072        None                   8192  \n",
       "15             Meta    True          131072        None                   8192  \n",
       "7              Groq    True          131072        None                   8192  \n",
       "14    Alibaba Cloud    True          131072        None                 131072  \n",
       "20             Groq    True          131072        None                   8192  \n",
       "4            PlayAI    True            8192        None                   8192  \n",
       "5            PlayAI    True            8192        None                   8192  \n",
       "11       Mistral AI    True           32768        None                  32768  \n",
       "19  DeepSeek / Meta    True          131072        None                 131072  \n",
       "13            SDAIA    True            4096        None                   4096  \n",
       "6              Meta    True          131072        None                  32768  \n",
       "21           OpenAI    True             448        None                    448  \n",
       "2              Meta    True            8192        None                   8192  \n",
       "1            Google    True            8192        None                   8192  \n",
       "12     Hugging Face    True             448        None                    448  \n",
       "9              Meta    True            8192        None                   8192  \n",
       "8            OpenAI    True             448        None                    448  \n",
       "18             Meta    True          131072        None                 131072  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "pd.DataFrame(response.json()['data']).sort_values(['created'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b438489-c102-434b-872d-e807169deef6",
   "metadata": {
    "id": "2b438489-c102-434b-872d-e807169deef6"
   },
   "source": [
    "The Groq client object enables interaction with the Groq REST API and a chat completion request is made via the client.chat.completions.create method.\n",
    "\n",
    "The most important arguments of the client.chat.completions.create method are the following:\n",
    "* messages: a list of messages (dictionary form) that make up the conversation to date\n",
    "* model: a string indicating which model to use (see [list of models](https://console.groq.com/docs/models))\n",
    "* max_completion_tokens: the maximum number of tokens that are generated in the chat completion\n",
    "* response_format: setting this to `{ \"type\": \"json_object\" }` enables JSON output\n",
    "* seed: sample deterministically as best as possible, though identical outputs each time are not guaranteed\n",
    "* temperature: between 0 and 2 where higher values like 0.8 make the output more random (creative) and values like 0.2 are more focused and deterministic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99216392-acc3-4a00-826b-c166a1e52534",
   "metadata": {
    "id": "99216392-acc3-4a00-826b-c166a1e52534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create in module groq.resources.chat.completions:\n",
      "\n",
      "create(*, messages: 'Iterable[ChatCompletionMessageParam]', model: \"Union[str, Literal['gemma2-9b-it', 'llama-3.3-70b-versatile', 'llama-3.1-8b-instant', 'llama-guard-3-8b', 'llama3-70b-8192', 'llama3-8b-8192']]\", exclude_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, frequency_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, function_call: 'Optional[completion_create_params.FunctionCall] | NotGiven' = NOT_GIVEN, functions: 'Optional[Iterable[completion_create_params.Function]] | NotGiven' = NOT_GIVEN, include_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, logit_bias: 'Optional[Dict[str, int]] | NotGiven' = NOT_GIVEN, logprobs: 'Optional[bool] | NotGiven' = NOT_GIVEN, max_completion_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, max_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, metadata: 'Optional[Dict[str, str]] | NotGiven' = NOT_GIVEN, n: 'Optional[int] | NotGiven' = NOT_GIVEN, parallel_tool_calls: 'Optional[bool] | NotGiven' = NOT_GIVEN, presence_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, reasoning_effort: \"Optional[Literal['none', 'default']] | NotGiven\" = NOT_GIVEN, reasoning_format: \"Optional[Literal['hidden', 'raw', 'parsed']] | NotGiven\" = NOT_GIVEN, response_format: 'Optional[completion_create_params.ResponseFormat] | NotGiven' = NOT_GIVEN, search_settings: 'Optional[completion_create_params.SearchSettings] | NotGiven' = NOT_GIVEN, seed: 'Optional[int] | NotGiven' = NOT_GIVEN, service_tier: \"Optional[Literal['auto', 'on_demand', 'flex']] | NotGiven\" = NOT_GIVEN, stop: 'Union[Optional[str], List[str], None] | NotGiven' = NOT_GIVEN, store: 'Optional[bool] | NotGiven' = NOT_GIVEN, stream: 'Optional[Literal[False]] | Literal[True] | NotGiven' = NOT_GIVEN, temperature: 'Optional[float] | NotGiven' = NOT_GIVEN, tool_choice: 'Optional[ChatCompletionToolChoiceOptionParam] | NotGiven' = NOT_GIVEN, tools: 'Optional[Iterable[ChatCompletionToolParam]] | NotGiven' = NOT_GIVEN, top_logprobs: 'Optional[int] | NotGiven' = NOT_GIVEN, top_p: 'Optional[float] | NotGiven' = NOT_GIVEN, user: 'Optional[str] | NotGiven' = NOT_GIVEN, extra_headers: 'Headers | None' = None, extra_query: 'Query | None' = None, extra_body: 'Body | None' = None, timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN) -> 'ChatCompletion | Stream[ChatCompletionChunk]' method of groq.resources.chat.completions.Completions instance\n",
      "    Creates a model response for the given chat conversation.\n",
      "\n",
      "    Args:\n",
      "      messages: A list of messages comprising the conversation so far.\n",
      "\n",
      "      model: ID of the model to use. For details on which models are compatible with the Chat\n",
      "          API, see available [models](https://console.groq.com/docs/models)\n",
      "\n",
      "      exclude_domains: Deprecated: Use search_settings.exclude_domains instead. A list of domains to\n",
      "          exclude from the search results when the model uses a web search tool.\n",
      "\n",
      "      frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n",
      "          existing frequency in the text so far, decreasing the model's likelihood to\n",
      "          repeat the same line verbatim.\n",
      "\n",
      "      function_call: Deprecated in favor of `tool_choice`.\n",
      "\n",
      "          Controls which (if any) function is called by the model. `none` means the model\n",
      "          will not call a function and instead generates a message. `auto` means the model\n",
      "          can pick between generating a message or calling a function. Specifying a\n",
      "          particular function via `{\"name\": \"my_function\"}` forces the model to call that\n",
      "          function.\n",
      "\n",
      "          `none` is the default when no functions are present. `auto` is the default if\n",
      "          functions are present.\n",
      "\n",
      "      functions: Deprecated in favor of `tools`.\n",
      "\n",
      "          A list of functions the model may generate JSON inputs for.\n",
      "\n",
      "      include_domains: Deprecated: Use search_settings.include_domains instead. A list of domains to\n",
      "          include in the search results when the model uses a web search tool.\n",
      "\n",
      "      logit_bias: This is not yet supported by any of our models. Modify the likelihood of\n",
      "          specified tokens appearing in the completion.\n",
      "\n",
      "      logprobs: This is not yet supported by any of our models. Whether to return log\n",
      "          probabilities of the output tokens or not. If true, returns the log\n",
      "          probabilities of each output token returned in the `content` of `message`.\n",
      "\n",
      "      max_completion_tokens: The maximum number of tokens that can be generated in the chat completion. The\n",
      "          total length of input tokens and generated tokens is limited by the model's\n",
      "          context length.\n",
      "\n",
      "      max_tokens: Deprecated in favor of `max_completion_tokens`. The maximum number of tokens\n",
      "          that can be generated in the chat completion. The total length of input tokens\n",
      "          and generated tokens is limited by the model's context length.\n",
      "\n",
      "      metadata: This parameter is not currently supported.\n",
      "\n",
      "      n: How many chat completion choices to generate for each input message. Note that\n",
      "          the current moment, only n=1 is supported. Other values will result in a 400\n",
      "          response.\n",
      "\n",
      "      parallel_tool_calls: Whether to enable parallel function calling during tool use.\n",
      "\n",
      "      presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on\n",
      "          whether they appear in the text so far, increasing the model's likelihood to\n",
      "          talk about new topics.\n",
      "\n",
      "      reasoning_effort: this field is only available for qwen3 models. Set to 'none' to disable\n",
      "          reasoning. Set to 'default' or null to let Qwen reason.\n",
      "\n",
      "      reasoning_format: Specifies how to output reasoning tokens\n",
      "\n",
      "      response_format: An object specifying the format that the model must output. Setting to\n",
      "          `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs\n",
      "          which ensures the model will match your supplied JSON schema. json_schema\n",
      "          response format is only supported on llama 4 models. Setting to\n",
      "          `{ \"type\": \"json_object\" }` enables the older JSON mode, which ensures the\n",
      "          message the model generates is valid JSON. Using `json_schema` is preferred for\n",
      "          models that support it.\n",
      "\n",
      "      search_settings: Settings for web search functionality when the model uses a web search tool.\n",
      "\n",
      "      seed: If specified, our system will make a best effort to sample deterministically,\n",
      "          such that repeated requests with the same `seed` and parameters should return\n",
      "          the same result. Determinism is not guaranteed, and you should refer to the\n",
      "          `system_fingerprint` response parameter to monitor changes in the backend.\n",
      "\n",
      "      service_tier: The service tier to use for the request. Defaults to `on_demand`.\n",
      "\n",
      "          - `auto` will automatically select the highest tier available within the rate\n",
      "            limits of your organization.\n",
      "          - `flex` uses the flex tier, which will succeed or fail quickly.\n",
      "\n",
      "      stop: Up to 4 sequences where the API will stop generating further tokens. The\n",
      "          returned text will not contain the stop sequence.\n",
      "\n",
      "      store: This parameter is not currently supported.\n",
      "\n",
      "      stream: If set, partial message deltas will be sent. Tokens will be sent as data-only\n",
      "          [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n",
      "          as they become available, with the stream terminated by a `data: [DONE]`\n",
      "          message. [Example code](/docs/text-chat#streaming-a-chat-completion).\n",
      "\n",
      "      temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n",
      "          make the output more random, while lower values like 0.2 will make it more\n",
      "          focused and deterministic. We generally recommend altering this or top_p but not\n",
      "          both.\n",
      "\n",
      "      tool_choice: Controls which (if any) tool is called by the model. `none` means the model will\n",
      "          not call any tool and instead generates a message. `auto` means the model can\n",
      "          pick between generating a message or calling one or more tools. `required` means\n",
      "          the model must call one or more tools. Specifying a particular tool via\n",
      "          `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n",
      "          call that tool.\n",
      "\n",
      "          `none` is the default when no tools are present. `auto` is the default if tools\n",
      "          are present.\n",
      "\n",
      "      tools: A list of tools the model may call. Currently, only functions are supported as a\n",
      "          tool. Use this to provide a list of functions the model may generate JSON inputs\n",
      "          for. A max of 128 functions are supported.\n",
      "\n",
      "      top_logprobs: This is not yet supported by any of our models. An integer between 0 and 20\n",
      "          specifying the number of most likely tokens to return at each token position,\n",
      "          each with an associated log probability. `logprobs` must be set to `true` if\n",
      "          this parameter is used.\n",
      "\n",
      "      top_p: An alternative to sampling with temperature, called nucleus sampling, where the\n",
      "          model considers the results of the tokens with top_p probability mass. So 0.1\n",
      "          means only the tokens comprising the top 10% probability mass are considered. We\n",
      "          generally recommend altering this or temperature but not both.\n",
      "\n",
      "      user: A unique identifier representing your end-user, which can help us monitor and\n",
      "          detect abuse.\n",
      "\n",
      "      extra_headers: Send extra headers\n",
      "\n",
      "      extra_query: Add additional query parameters to the request\n",
      "\n",
      "      extra_body: Add additional JSON properties to the request\n",
      "\n",
      "      timeout: Override the client-level default timeout for this request, in seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.chat.completions.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7",
   "metadata": {
    "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7"
   },
   "source": [
    "As a first example, note how the messages input is given as a list of a dictionaries with `role` and `content` keys. This is in a ChatML format recognised by many LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81",
   "metadata": {
    "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models work by:\n",
      "\n",
      "1. **Training**: They're fed vast amounts of text data, which helps them learn patterns and relationships between words.\n",
      "2. **Tokenization**: The model breaks down text into smaller units (tokens) like words or characters.\n",
      "3. **Contextualization**: It analyzes the tokens and their context to predict the next token, creating a probability distribution.\n",
      "4. **Generation**: The model uses this probability distribution to generate text, one token at a time, based on the input prompt or context.\n",
      "\n",
      "This process relies on complex algorithms and neural networks, allowing the model to understand and generate human-like language.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {   \"role\": \"system\", # sets the persona of the model\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": \"Explain briefly how large language models work\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b60c2-4300-4338-b325-c3ea876c1afe",
   "metadata": {
    "id": "728b60c2-4300-4338-b325-c3ea876c1afe"
   },
   "source": [
    "The output is in Markdown format so the following line formats this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88845bb3-4a3e-403a-93ab-439c46aa1832",
   "metadata": {
    "id": "88845bb3-4a3e-403a-93ab-439c46aa1832"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Large language models work by:\n",
       "\n",
       "1. **Training**: They're fed vast amounts of text data, which helps them learn patterns and relationships between words.\n",
       "2. **Tokenization**: The model breaks down text into smaller units (tokens) like words or characters.\n",
       "3. **Contextualization**: It analyzes the tokens and their context to predict the next token, creating a probability distribution.\n",
       "4. **Generation**: The model uses this probability distribution to generate text, one token at a time, based on the input prompt or context.\n",
       "\n",
       "This process relies on complex algorithms and neural networks, allowing the model to understand and generate human-like language."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8c97b-1711-4355-b108-86bed769c109",
   "metadata": {
    "id": "e1c8c97b-1711-4355-b108-86bed769c109"
   },
   "source": [
    "## Text summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4",
   "metadata": {
    "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4"
   },
   "source": [
    "We start with a llama3-8b-8192, a model using just over 8 billion parameters with at most 8192 tokens produced as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2",
   "metadata": {
    "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2"
   },
   "source": [
    "Here is an article to be summarised from the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29",
   "metadata": {
    "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29"
   },
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco area Friday at 4:42 a.m. PT (7:42 a.m. ET), the U.S. Geological Survey reported. The quake left about 2,000 customers without power, said David Eisenhower, a spokesman for Pacific Gas and Light. Under the USGS classification, a magnitude 4.2 earthquake is considered \"light,\" which it says usually causes minimal damage. \"We had quite a spike in calls, mostly calls of inquiry, none of any injury, none of any damage that was reported,\" said Capt. Al Casciato of the San Francisco police. \"It was fairly mild.\" Watch police describe concerned calls immediately after the quake » . The quake was centered about two miles east-northeast of Oakland, at a depth of 3.6 miles, the USGS said. Oakland is just east of San Francisco, across San Francisco Bay. An Oakland police dispatcher told CNN the quake set off alarms at people's homes. The shaking lasted about 50 seconds, said CNN meteorologist Chad Myers. According to the USGS, magnitude 4.2 quakes are felt indoors and may break dishes and windows and overturn unstable objects. Pendulum clocks may stop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754257a-74ff-4747-a18e-7ebfbc82639c",
   "metadata": {
    "id": "9754257a-74ff-4747-a18e-7ebfbc82639c"
   },
   "source": [
    "**Exercise:**\n",
    "Summarise the story text using the following three prompts. Use the format given above but here there is no need to set the persona (i.e. only include one dictionary in the messages list when calling `client.chat.completions.create`.) Comment on any differences.\n",
    "\n",
    "1) \"Summarise the following article in 3 sentences.\"\n",
    "\n",
    "2) \"Give me a TL;DR of this text.\"\n",
    "\n",
    "3) \"What's the key takeaway here?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c372155-bdde-45a4-a184-fce4c4e8034c",
   "metadata": {
    "id": "9c372155-bdde-45a4-a184-fce4c4e8034c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarise the following article in 3 sentences.  \n",
      " Here is a summary of the article in 3 sentences:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area at 4:42 a.m. PT on Friday, causing minimal damage and no reported injuries. The quake left around 2,000 customers without power, but Pacific Gas and Light said most of the calls they received were inquiries rather than reports of damage. The earthquake, which was centered about two miles east-northeast of Oakland, lasted for about 50 seconds and was felt indoors, although it did trigger some alarm systems and cause slight shaking.\n",
      "Give me a TL;DR of this text.  \n",
      " A magnitude 4.2 earthquake struck the San Francisco area at 4:42am, causing minimal damage and power outages. The quake, centered near Oakland, lasted about 50 seconds and was felt indoors, but no injuries or significant damage were reported.\n",
      "What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake struck the San Francisco area early Friday morning, causing minimal damage and no reported injuries, with most callers inquiring about the event rather than reporting damage or emergencies.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Summarise the following article in 3 sentences. \", \"Give me a TL;DR of this text. \", \"What's the key takeaway here?\"]\n",
    "#content will be p + story for p in prompts\n",
    "\n",
    "# ANSWER\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": p + story}]\n",
    ")\n",
    "\n",
    "    print(p, '\\n', response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df",
   "metadata": {
    "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df"
   },
   "source": [
    "Run the above code again below and note that the answers may differ. This is due to the probabilistic nature of LLM token generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14",
   "metadata": {
    "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarise the following article in 3 sentences.  \n",
      " Here is a 3 sentence summary of the article:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area at 4:42 a.m. PT on Friday, causing minimal damage and no reported injuries. The quake, which was centered about two miles east-northeast of Oakland, affected around 2,000 customers who lost power, but most of the calls received by authorities were inquiries rather than reports of damage or injury. The earthquake was classified as \"light\" by the USGS, with effects such as shaking that lasted about 50 seconds and caused slight disruptions, like setting off home alarms and potentially breaking dishes or windows.\n",
      "Give me a TL;DR of this text.  \n",
      " A magnitude 4.2 earthquake struck the San Francisco area at 4:42am PT, causing minimal damage and no reported injuries. The quake was centered near Oakland, about 2 miles east-northeast, and lasted for about 50 seconds. Approximately 2,000 customers were left without power, but the majority of reports were simply of concerned residents calling to inquire about the quake's severity.\n",
      "What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake struck the San Francisco area, causing no reported injuries or significant damage, but leaving around 2,000 customers without power and setting off alarms at homes.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": p + story}]\n",
    ")\n",
    "\n",
    "    print(p, '\\n', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a39af-3722-4b71-8d70-cbfa63593496",
   "metadata": {
    "id": "405a39af-3722-4b71-8d70-cbfa63593496"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f",
   "metadata": {
    "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f"
   },
   "source": [
    "## Text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf",
   "metadata": {
    "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf"
   },
   "source": [
    "**Exercise**: In this section adjust the `max_completion_tokens` and `temperature` settings below to obtain different responses. Show some examples with the prompt \"Continue the story: It was a great time to be alive\" with the model \"llama-3.1-8b-instant\".\n",
    "\n",
    "* max_completion_tokens - the maximum number of tokens to generate. Note that longer words are made of multiple tokens (set to 200 and 500)\n",
    "* temperature (positive number) - the higher the number the more random (creative) the output (set to 0.2, 0.8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50207704-94be-4309-8a82-dc1d22a063ee",
   "metadata": {
    "id": "50207704-94be-4309-8a82-dc1d22a063ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People of all ages filled the streets, laughing and chatting with one another. The air was filled with the sweet scent of blooming flowers and the distant sound of music drifting from the park. It was a great time to be alive, and everyone knew it.\n",
      "\n",
      "Rachel, a young woman with a bright smile and infectious energy, had just finished her shift at the local coffee shop. She was sipping on a cold glass of lemonade and enjoying the warm sunshine on her face. As she strolled down the street, she noticed a group of musicians setting up their equipment in the park.\n",
      "\n",
      "One of the musicians, a charming guitarist with a messy mop of hair, caught her eye. He flashed her a warm smile, and Rachel couldn't help but feel drawn to him. As the music began to play, Rachel found herself swaying to the rhythm, mesmerized by the guitarist's talents.\n",
      "\n",
      "The music was a lively mix of folk and rock, with catchy melodies and heartfelt lyrics. Rachel couldn't\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=200, do not have a temperature setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    max_completion_tokens=200,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63235fcc-3a61-4829-998b-37f2f104322b",
   "metadata": {
    "id": "63235fcc-3a61-4829-998b-37f2f104322b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As I walked through the vibrant streets of the city, surrounded by people from all walks of life, I couldn't help but feel a sense of excitement and wonder. The year was 2050, and the world had changed beyond recognition. Towering skyscrapers made of shimmering metals and sustainable materials pierced the sky, their exteriors a mesmerizing display of color and light. Flying cars zoomed by, their humming engines a familiar soundtrack to everyday life.\n",
      "\n",
      "I stopped at a street vendor, who offered me a sample of the latest culinary innovation: lab-grown, nutrient-rich \"food cubes\" that tasted like anything I desired. I chose the flavor of a juicy burger, and took a bite. The explosion of flavors was incredible, and I couldn't believe how real it tasted. The vendor smiled and said, \"Welcome to the future, my friend.\"\n",
      "\n",
      "I continued my stroll, taking in the sights and sounds of this brave new world. Everywhere I looked, I saw people from all over the globe coming together, united in their pursuit of progress and happiness. The city was a melting pot of cultures, with languages and traditions blending together in a beautiful tapestry of diversity.\n",
      "\n",
      "As I turned a corner, I stumbled upon a massive public square, filled with people of all ages and backgrounds. They were engaged in a lively debate, their faces illuminated by augmented reality contact lenses that displayed information and statistics in real-time. The air was filled with the hum of holographic projections, which swirled and danced above the crowd like a mad whirlwind of color and light.\n",
      "\n",
      "I watched in awe as the debate raged on, topics ranging from climate change to robotics and artificial intelligence. It was a time of great division and great unity, as people came together to discuss the challenges and opportunities of this new world.\n",
      "\n",
      "Suddenly, a figure emerged from the crowd, drawing everyone's attention. It was a young woman, her eyes shining with a fierce intensity. She held a small device in her hand, and her words echoed across the square, carried by the augmented reality contact lenses of the crowd.\n",
      "\n",
      "\"We have reached a turning point in human history,\" she declared. \"The choices we make now will determine the course of our future. Will we continue down the path of growth and progress, or will we succumb to the dangers of technological singularity? The decision is ours, and ours alone.\"\n",
      "\n",
      "The crowd erupted in a chorus of cheers and applause, with some people cheering for progress and others\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=500, do not have a temperature setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    max_completion_tokens=500,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50",
   "metadata": {
    "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive. The sun was shining brightly in the clear blue sky, casting a warm glow over the bustling streets of the city. People of all ages and backgrounds were out and about, enjoying the beautiful day and the sense of community that came with it.\n",
      "\n",
      "Lena, a young woman with a bright smile and a contagious laugh, was walking down the street, feeling carefree and alive. She had just finished a long week of work and was looking forward to a well-deserved break. As she strolled along, she noticed the vibrant street art that adorned the buildings, the smell of freshly baked bread wafting from the nearby bakery, and the sound of children's laughter echoing from the park.\n",
      "\n",
      "She stopped at a small café to grab a coffee and people-watch. The café was filled with the sounds of lively chatter and the aroma of freshly brewed coffee. Lena took a seat at a small table by the window and watched as people of all ages and backgrounds walked by, each with their own unique story to tell.\n",
      "\n",
      "As she sipped her coffee, Lena noticed a young musician setting up his guitar on the street corner. He began to play a soulful melody that seemed to capture the essence of the moment. People stopped to listen, mesmerized by the beauty of the music. Lena felt her heart swell with emotion as she listened, feeling a deep connection to the music and the people around her.\n",
      "\n",
      "The musician finished his song to applause and cheers from the crowd. Lena joined in, clapping along with the others. As she did, she felt a sense of belonging and connection to the people around her. It was a moment that would stay with her forever, a reminder of the beauty and joy that could be found in the simplest moments of life.\n",
      "\n",
      "As the musician packed up his guitar and continued on his way, Lena felt a sense of gratitude for this moment in time. It was a great time to be alive, and she was grateful to be a part of it. She finished her coffee, feeling refreshed and renewed, and continued on her way, ready to take on the day with a sense of wonder and curiosity.\n",
      "\n",
      "But little did Lena know, her life was about to take a dramatic turn. As she walked away from the café, she stumbled upon a small, mysterious shop that she had never noticed before. The sign above the door read \"Curios and Wonders,\" and the windows were filled with a dazzling array of strange and exotic objects. Lena felt a sense of curiosity and intrigue wash over her as she pushed open the door and stepped inside...\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 0.2, do not have a max_completion_tokens setting)\n",
    "# ANSWER (set temperature = 0.2, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive.\"}],\n",
    "    temperature = 0.2,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d",
   "metadata": {
    "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun was shining bright, casting a warm glow over the bustling streets. People from all walks of life were out and about, laughing, chatting, and soaking up the vibrant atmosphere. The air was filled with the sweet scent of blooming flowers and the sound of music drifting through the air.\n",
      "\n",
      "As I walked through the city, I couldn't help but feel a sense of excitement and possibility. It was a time of great change and progress, and everyone seemed to be caught up in the momentum.\n",
      "\n",
      "I passed by a group of young artists, gathered around a street performer who was juggling clubs and spinning plates to the delight of the crowd. Nearby, a group of activists were setting up a stall, calling for greater awareness about social justice and equality.\n",
      "\n",
      "Further down the street, I saw a line of people waiting to get into a popular café, where a new exhibition was about to open. The café was known for its eclectic mix of art, music, and politics, and it seemed to be the hub of the city's creative and intellectual scene.\n",
      "\n",
      "As I made my way through the crowd, I felt a tap on my shoulder. It was an old friend, Emma, who had just moved back to the city after years abroad. We hugged warmly and caught up on each other's lives, exchanging stories about our adventures and our plans for the future.\n",
      "\n",
      "\"You look amazing,\" Emma said, eyeing my brightly colored outfit. \"I love the way you always dress with such style and flair.\"\n",
      "\n",
      "\"Thanks, you're looking pretty fabulous yourself,\" I replied, grinning at her eclectic mix of patterns and textures. \"I love your scarf – where did you get it?\"\n",
      "\n",
      "We spent the next hour chatting and laughing, catching up on each other's lives and sharing stories about our experiences abroad. As we walked through the city, we stumbled upon a small park that was hosting a free outdoor concert. We joined in, dancing and singing along with the other dancers, feeling carefree and alive.\n",
      "\n",
      "It was a truly magical day, one that I would never forget. As we parted ways at the end of the evening, Emma turned to me and said, \"This is the best time to be alive, don't you think?\"\n",
      "\n",
      "I smiled, feeling a sense of agreement with her. \"Absolutely,\" I said. \"There's so much to look forward to, so many possibilities and opportunities to explore.\"\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 1, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive.\"}],\n",
    "    temperature = 1,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf",
   "metadata": {
    "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf"
   },
   "source": [
    "Note what happens when the temperature is set too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "931f8e00-928c-4218-8e74-8c56269fbfcb",
   "metadata": {
    "id": "931f8e00-928c-4218-8e74-8c56269fbfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive, the smell of freshly brewed coffee wafting from the coffee cart outside, the sun peeking over the buildings, lighting up the world in shades of golden orange, and the city buzzing with excitement.\n",
      "\n",
      "People of all ages flocked to the central square for the grand festival taking over the town. The event organizers set a majestic stage amidst the vibrant atmosphere as they readied the evening’s headlining attraction.\n",
      "\n",
      "In the heart of the excitement lay one such vibrant soul: A talented young musician with bright ambition named Maya. Born in these beautiful surroundings, her entire musical life took birth here and this festival would provide the opportunity to launch and shine to all that it had taught them in such beautiful musical places that surrounded their musical life so vivid so vibrant with every note so true that all her beautiful melodies sang about all that her home offered. This evening performance could bring everything it held together in their music.\n",
      "\n",
      "Years back during the festival of love which they used to love attending it had been the day when it sparked a beautiful fire within which lit a passion within this music filled town. Her father the great maester once a humble and very kind man would play a magical song in tune which would echo and bring in such peace and silence among a crowd that they would all fall at your feet listening for a little longer just wanting it to prolong a beautiful feeling to never fade, never get weak. It took time but Maya grew up to master every instrument he left and was eager and was to create the same feelings that she heard all those many nights long ago in these grand festival filled evenings.\n",
      "\n",
      "Tonight as night begins it's time that Maya finally stepped onto those majestic stages amidst this beautiful atmosphere of excitement filled nights in which the stars shone a little brighter so the people and so the musician on top would have enough courage and enough hope and energy of love for those souls around her that would gather once more here in these beautiful grounds, so as the night takes flight, the dream and magic she sang for, takes birth with hope. The night, it is her time. It awaits in all beauty as it lights the grand, it is here she stands as music is ready... the world will see if this grand performance would bring what magic that they knew would forever and never fade but will stay alive like one that beats in your every heart.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 2, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive.\"}],\n",
    "    temperature = 2,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c",
   "metadata": {
    "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c"
   },
   "source": [
    "### Zero-shot and one-short prompting for question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6662f0a-c7e3-4235-972d-2771781a4e53",
   "metadata": {
    "id": "f6662f0a-c7e3-4235-972d-2771781a4e53"
   },
   "source": [
    "This section shows the impact of prompting on the response. Zero-shot prompting means we provide the prompt without any examples or additional context. Let us initially ask Mistral a question using no prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc",
   "metadata": {
    "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chemical reactions occur when two or more substances, known as reactants, interact with each other and transform into new substances, known as products. The process of chemical reaction involves the breaking and forming of chemical bonds between atoms.\n",
       "\n",
       "Here's a simplified overview of how two chemicals react:\n",
       "\n",
       "1. **Interaction**: Two chemicals, A and B, come into contact with each other. This can happen through a variety of mechanisms, such as mixing, diffusion, or collision.\n",
       "2. **Bond breaking**: The atoms in chemical A and B begin to interact with each other, causing the existing bonds within each molecule to break. This process is called bond cleavage.\n",
       "3. **Bond formation**: As the bonds within each molecule break, new bonds begin to form between the atoms of chemicals A and B. This process is called bond formation or covalent bonding.\n",
       "4. **Reaction mechanisms**: The specific pathway that the reaction takes is called the reaction mechanism. This can involve a series of intermediate molecules that form and then disappear as the reaction progresses.\n",
       "5. **Product formation**: The final products of the reaction are formed when the bonds have fully rearranged, resulting in a new set of compounds.\n",
       "\n",
       "There are different types of chemical reactions, including:\n",
       "\n",
       "1. **Combustion reaction**: A reaction that involves the burning of a substance, typically with oxygen.\n",
       "2. **Synthesis reaction**: A reaction in which two or more substances combine to form a new compound.\n",
       "3. **Decomposition reaction**: A reaction in which a single substance breaks down into two or more simpler substances.\n",
       "4. **Replacement reaction**: A reaction in which one element or group of elements replaces another within a compound.\n",
       "5. **Neutralization reaction**: A reaction in which an acid reacts with a base to form a salt and water.\n",
       "\n",
       "To illustrate this, let's consider a simple example:\n",
       "\n",
       "**Reaction:** 2H2 (hydrogen gas) + O2 (oxygen gas) → 2H2O (water)\n",
       "\n",
       "In this reaction, the hydrogen gas (H2) and oxygen gas (O2) interact, breaking their existing bonds and forming new bonds to create water (H2O). This is a simple example of a synthesis reaction.\n",
       "\n",
       "Keep in mind that chemical reactions are governed by the laws of thermodynamics and involve a change in energy. The overall change in energy can be either exergonic (releases energy) or endergonic (absorbs energy)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d",
   "metadata": {
    "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d"
   },
   "source": [
    "**Exercise:** Ask the same question but modify the prompt to return the answer to the same question in a simpler form (still using the llama-3.1-8b-instant model). Experiment with different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6",
   "metadata": {
    "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's talk about chemicals.\n",
       "\n",
       "Chemicals are like special kinds of friends that can mix and match with each other. When they mix, they can do different things together. This is called a chemical reaction.\n",
       "\n",
       "Imagine you have two buckets, one with blue paint and one with yellow paint. When you mix them together, what do you get? That's right, you get green paint!\n",
       "\n",
       "Chemicals work in a similar way. When two chemicals mix, they can create something new, like a different color or a new smell. Sometimes, they can even make something that didn't exist before.\n",
       "\n",
       "Let's say we have two chemicals, A and B. When they mix, they create a new chemical, C. This is like making a new friend, but instead of a person, it's a chemical.\n",
       "\n",
       "Chemical reactions can be slow or fast, and they can happen in a lot of different ways. Sometimes, they can even produce heat or light. But don't worry, most of the time, chemical reactions happen in a way that's safe and fun.\n",
       "\n",
       "So, that's what chemical reactions are like! It's like mixing and matching special friends to create something new and cool."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Answer the following question as though I am 10 years old. How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04676a11-57a7-4f9c-87e2-128559f0ffce",
   "metadata": {
    "id": "04676a11-57a7-4f9c-87e2-128559f0ffce"
   },
   "source": [
    "### One-shot prompting ###\n",
    "\n",
    "Next, note the dramatic change when we give the following template setting a new role and providing an English question followed by a French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473",
   "metadata": {
    "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment réagissent deux chimiques entre elles.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"system\",\n",
    "             \"content\": \"You translate English to French.\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"What time is it?\"},\n",
    "               {\"role\": \"assistant\",\n",
    "               \"content\": \"Quelle heure est-il?\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269",
   "metadata": {
    "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269"
   },
   "source": [
    "### Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050",
   "metadata": {
    "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050"
   },
   "source": [
    "Recall that since the text generation process outputs one token at a time, their outputs often need adjusting. This is where examples can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b",
   "metadata": {
    "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I regret to inform you that I will be unable to attend the meeting. Apologies for any inconvenience this may cause.\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"I'm gonna head out now, see you later.\"\n",
    "response1 = \"I will be leaving now. See you later.\"\n",
    "\n",
    "prompt2 =  \"That movie was super cool!\"\n",
    "response2 = \"The movie was very impressive.\"\n",
    "\n",
    "prompt3 = \"Can't make it to the meeting, sorry.\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a professional editor. Rewrite casual sentences into a formal tone.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e9e18-cc27-453b-8844-3f683fb607f8",
   "metadata": {
    "id": "040e9e18-cc27-453b-8844-3f683fb607f8"
   },
   "source": [
    "The output can also be moulded to provide SQL output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed854534-30db-4745-b910-27d12a3ed47e",
   "metadata": {
    "id": "ed854534-30db-4745-b910-27d12a3ed47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM products WHERE quantity_in_stock = 0;\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"Show me all users who signed up in the last 30 days.\"\n",
    "response1 = \"SELECT * FROM users WHERE signup_date >= CURRENT_DATE - INTERVAL '30 days';\"\n",
    "\n",
    "prompt2 = \"What is the average order value?\"\n",
    "response2 =  \"SELECT AVG(order_total) FROM orders;\"\n",
    "\n",
    "prompt3 = \"List products that are out of stock.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that translates natural language to SQL.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7",
   "metadata": {
    "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7"
   },
   "source": [
    "**Exercise**: Create a few examples to train the \"llama3-70b-8192\" LLM to take in user content in the form below and provide output as a pandas dataframe. Use the `exec` function to execute its output to display the answer of sample input as a data frame.\n",
    "\n",
    "Example:\n",
    "\n",
    "given the user content\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "| col1 | col2 | col3\n",
    "\n",
    "| 32 | 27 | 25\n",
    "\n",
    "| 64 | 23 | 14\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train the model to output\n",
    "\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b8ac08a-5759-41cd-a560-e77694573723",
   "metadata": {
    "id": "2b8ac08a-5759-41cd-a560-e77694573723"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colA</th>\n",
       "      <th>colB</th>\n",
       "      <th>colC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   colA  colB  colC\n",
       "0    23    12    54\n",
       "1     8    76    32\n",
       "2     7     5     3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANSWER\n",
    "user1 = \"\"\"col1 | col2 | col3\n",
    "32 | 27 | 25\n",
    "64 | 23 | 14\n",
    "\"\"\"\n",
    "\n",
    "output1 = \"\"\"\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "\"\"\"\n",
    "\n",
    "user2 = \"\"\"col1 | col2\n",
    "23 | 12\n",
    "8 | 76\n",
    "7 | 5\n",
    "\"\"\"\n",
    "output2 = \"\"\"\n",
    "df = pd.DataFrame({'col1': [23, 8, 7], 'col2': [12, 76, 5]})\n",
    "\"\"\"\n",
    "user3 = \"\"\"colA | colB | colC\n",
    "23 | 12 | 54\n",
    "8 | 76 | 32\n",
    "7 | 5 | 3\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a data scientist who will receive data input as a string and provide output as a pandas dataframe called df. Use the examples to guide you\"},\n",
    "        {\"role\": \"user\", \"content\": user1},\n",
    "        {\"role\": \"assistant\", \"content\": output1},\n",
    "        {\"role\": \"user\", \"content\": user2},\n",
    "        {\"role\": \"user\", \"content\": output2},\n",
    "        {\"role\": \"user\", \"content\": user3}\n",
    "    ]\n",
    ")\n",
    "\n",
    "exec(response.choices[0].message.content.strip()) # string executed as Python code\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f62a88-c7b4-4431-acda-97e9a98981f4",
   "metadata": {
    "id": "29f62a88-c7b4-4431-acda-97e9a98981f4"
   },
   "source": [
    "Also show what happens when the question is asked in the absence of a system role and without few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6",
   "metadata": {
    "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It looks like you provided a table with three columns: colA, colB, and colC, and three rows of data. Is there something specific you'd like to do with this data, such as filter, sort, or perform a calculation?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user3}\n",
    "    ]\n",
    ")\n",
    "response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660884-dfd4-4b96-a65d-d21ddbf99423",
   "metadata": {
    "id": "28660884-dfd4-4b96-a65d-d21ddbf99423"
   },
   "source": [
    "### Chain-of-thought prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f85ef-afc0-4e31-9ad8-ae780c535837",
   "metadata": {
    "id": "724f85ef-afc0-4e31-9ad8-ae780c535837"
   },
   "source": [
    "The results of question-answering can also be improved by prompting the LLM to provide intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee04ad6-968b-406a-8119-981b01ef5f92",
   "metadata": {
    "id": "bee04ad6-968b-406a-8119-981b01ef5f92"
   },
   "source": [
    "**Exercise**: Using the following prompts, compare the answers of the \"llama3-8b-8192\" model (set seed=21). (If this model is no longer available choose a model with relatively few parameters.)\n",
    "\n",
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb",
   "metadata": {
    "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------zero-shot-prompt------\n",
      "There are 2 s's in the word 'success'.\n",
      "------chain-of-thought------\n",
      "To count the number of 's's in the word \"success\", I will go through each letter one by one:\n",
      "\n",
      "1. S\n",
      "   There is 1 's' so far.\n",
      "\n",
      "2. U\n",
      "   There's an 'U' and still 1 's' so far.\n",
      "\n",
      "3. C\n",
      "   There's a 'C', and still 1 's' so far.\n",
      "\n",
      "4. C\n",
      "   There's another 'C', and still 1 's' so far.\n",
      "\n",
      "5. E\n",
      "   There's an 'E', and still 1 's' so far.\n",
      "\n",
      "6. S\n",
      "   There's another 's', which means there are 2 's's so far.\n",
      "\n",
      "7. S\n",
      "   There's a third 's', which means there are 3 's's so far.\n",
      "\n",
      "8. S\n",
      "   There's a fourth 's', which means there are 4 's's so far.\n",
      "\n",
      "There are 4 's's in the word \"success\".\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\"\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": zero_shot_prompt}],\n",
    "    seed = 21\n",
    ")\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": chain_of_thought_prompt}],\n",
    "    seed = 21\n",
    ")\n",
    "\n",
    "print('------zero-shot-prompt------')\n",
    "print(response1.choices[0].message.content)\n",
    "\n",
    "print('------chain-of-thought------')\n",
    "print(response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f",
   "metadata": {
    "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f"
   },
   "source": [
    "## Comparison of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f",
   "metadata": {
    "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f"
   },
   "source": [
    "**Exercise**: Compare the performance of 2 LLMs by outputting the answers of the following questions into a dataframe.\n",
    "\n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\",\n",
    "\n",
    "Column headings:\n",
    "\n",
    "Model Name | Question | Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238",
   "metadata": {
    "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist break up with the statistician? \\n\\nBecause they had too many p-values and not enough real-life applications! 😂 \\n\\n\\nLet me know if you'd like to hear another one! 😄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>Here's how to calculate 22 * 13 mentally using a few tricks:\\n\\n**1. Break it Down**\\n\\n* Think of 22 as (20 + 2). \\n* Now you have: (20 + 2) * 13\\n\\n**2. Distribute**\\n\\n* Multiply 20 by 13: 20 * 13 = 260\\n* Multiply 2 by 13: 2 * 13 = 26\\n\\n**3. Add the Results**\\n\\n*  260 + 26 = 286\\n\\n\\n**Therefore, 22 * 13 = 286**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>Pipkin didn't understand. Everyone was telling him \"sit up\", \"roll over\", \"smile\", but where was the fun in that? He much preferred the lie-flat-and-stargaze kind of fun.\\n\\nHis world was a kaleidoscope of colours - Mama's brightly patterned dress, the sunshine painting squares on the rug, the stripy blanket Auntie Clara had knitted. It was all so absorbing, so fascinating, that the urge to reach out, to explore, was bubbling inside him like a fizzy drink.\\n\\nOne bright morning, something happened. Pipkin's chubby hand brushed against a stray button on his rocking horse. For the first time, his hand scrabbled for something beyond the familiar expanse of his blanket. He reached, his other hand mimicking the movement in a clumsy mirror image. He pulled himself forward, his legs kicking erratically, his brow furrowed in concentration.\\n\\nHe screeched with triumphant joy, a gargling sound that always made his Mama giggle. He felt a surge of exhilaration, a thrill of conquest. \"Again!\" he seemed to be saying, though all that came out was a happy gurgle.\\n\\nAnd again he tried, again he pulled himself forward. Each inch was a victory, each wobbly attempt a lesson learned. His little limbs, propelled by a newfound determination, navigated the obstacles of the rug. His eyes, wide and full of wonder, tracked the path ahead.\\n\\nMama watched, her smile widening with every scrabble and stretch. She clapped her hands, cheering him on, \"That's it, Pipkin, you're doing it! Crawl, crawl, crawl!\"\\n\\nPipkin didn't know what \"crawl\" meant, but he understood the feeling. The feeling of movement, of control, of the world opening up before him. He crawled towards a brightly-coloured mobile hanging above his crib. He reached for it, his tiny fingers grasping at the dangling toys.\\n\\nHe didn't quite reach, but he didn't care. He had tasted freedom, the sweet freedom of movement. He had discovered the magic of crawling. And as he lay back on his rug, a happy sigh escaping his lips, Pipkin knew that this was only the beginning of his grand adventures. \\n\\n\\nThe world was his to explore, and he was ready to crawl.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist quit his job?\\n\\nBecause he couldn't regression to his old ways of making a living and had too many correlations to his previous complaints, but ultimately he just needed a feature upgrade in his life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>To calculate 22 * 13 mentally, you can use the technique of breaking down the numbers and then multiplying them partially.\\n\\nOne way to do it is:\\n\\n1. Break down the multiplication into easier parts: (20 * 13) + (2 * 13)\\n2. Calculate (20 * 13) = (20 * 10) + (20 * 3) \\n  (20 * 10) = 200 \\n  (20 * 3) = 60 \\n  So (20 * 13) = 200 + 60 = 260 \\n3. Then calculate (2 * 13)\\n  (2 * 10) = 20 \\n  (2 * 3) = 6 \\n  So (2 * 13) = 20 + 6 = 26 \\n4. Add both results: (260 + 26) = 286 \\n\\nTherefore, 22 * 13 = 286.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>**The Crawling Adventure**\\n\\nIn a cozy little house on a quiet street, a tiny miracle was waiting to unfold. Baby Emma, with her soft, fluffy hair and curious eyes, was about to embark on her most epic adventure yet: learning to crawl.\\n\\nEmma's earliest memories were of being held close by her mom, feeling the warmth and love of her touch. She'd gaze up at her mother's smiling face, watching as she played with her fingers, blew gentle kisses, and whispered sweet nothings. But as the days went by, Emma began to feel an inexplicable itch – a sense that there was more to explore, more to discover.\\n\\nOne morning, as the sun peeked through the windows, casting a warm glow over the room, Emma started to stir. She pushed herself up onto her forearms, her eyes scanning the surroundings as if searching for secrets hidden in the crevices of the furniture. Her mom, watching from a nearby chair, smiled knowingly.\\n\\n\"Today's the day!\" she whispered, setting down her book and creeping toward Emma.\\n\\nAt first, Emma was hesitant, her little hands grasping the soft blanket beneath her. But with each gentle urging from her mom, she began to shift, ever so slightly. Her legs twitched, her torso moved, and – in a moment of pure bliss – she launched herself forward, propelled by pure, unadulterated momentum.\\n\\nThe room seemed to spin as Emma crawled, her tiny hands grasping at the air, her feet wobbling in mid-air. Her mom laughed, her eyes shining with pride, as she carefully positioned herself beside Emma.\\n\\n\"Whoa, you're moving!\" she exclaimed, scooping up Emma in a triumphant hug.\\n\\nWith each try, Emma grew bolder, her confidence growing like a bloom of sunflowers in the spring. She inched closer to the bookshelf, fascinated by the towering stacks of picture books. She crept along the couch, exploring the crevices beneath the cushions. Even the carpet became a magical playground, as Emma discovered the delight of rolling onto her tummy, arms and legs splayed wide.\\n\\nOne afternoon, as the sun began to set, casting long shadows across the room, Emma reached the summit – a pile of colorful toys at the far end of the room. With a squeal of excitement, she launched herself toward the treasure trove, her tiny hands grasping for the soft, wiggly creatures. Her mom cheered her on, snapping photos to capture the moment.\\n\\nFrom that day forward, Emma's world expanded exponentially. Every step, every crawl, and every wobble became a thrilling adventure, a journey of discovery and growth. And as she looked up at her mom, beaming with pride, Emma knew she was no longer just a baby – she was a brave, fearless explorer, ready to take on the world.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model Name  \\\n",
       "0          gemma2-9b-it   \n",
       "1          gemma2-9b-it   \n",
       "2          gemma2-9b-it   \n",
       "3  llama-3.1-8b-instant   \n",
       "4  llama-3.1-8b-instant   \n",
       "5  llama-3.1-8b-instant   \n",
       "\n",
       "                                                 Question  \\\n",
       "0                      Tell me a joke about data science.   \n",
       "1                 How can one calculate 22 * 13 mentally?   \n",
       "2  Write a creative story about a baby learning to crawl.   \n",
       "3                      Tell me a joke about data science.   \n",
       "4                 How can one calculate 22 * 13 mentally?   \n",
       "5  Write a creative story about a baby learning to crawl.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Why did the data scientist break up with the statistician? \\n\\nBecause they had too many p-values and not enough real-life applications! 😂 \\n\\n\\nLet me know if you'd like to hear another one! 😄  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Here's how to calculate 22 * 13 mentally using a few tricks:\\n\\n**1. Break it Down**\\n\\n* Think of 22 as (20 + 2). \\n* Now you have: (20 + 2) * 13\\n\\n**2. Distribute**\\n\\n* Multiply 20 by 13: 20 * 13 = 260\\n* Multiply 2 by 13: 2 * 13 = 26\\n\\n**3. Add the Results**\\n\\n*  260 + 26 = 286\\n\\n\\n**Therefore, 22 * 13 = 286**  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Pipkin didn't understand. Everyone was telling him \"sit up\", \"roll over\", \"smile\", but where was the fun in that? He much preferred the lie-flat-and-stargaze kind of fun.\\n\\nHis world was a kaleidoscope of colours - Mama's brightly patterned dress, the sunshine painting squares on the rug, the stripy blanket Auntie Clara had knitted. It was all so absorbing, so fascinating, that the urge to reach out, to explore, was bubbling inside him like a fizzy drink.\\n\\nOne bright morning, something happened. Pipkin's chubby hand brushed against a stray button on his rocking horse. For the first time, his hand scrabbled for something beyond the familiar expanse of his blanket. He reached, his other hand mimicking the movement in a clumsy mirror image. He pulled himself forward, his legs kicking erratically, his brow furrowed in concentration.\\n\\nHe screeched with triumphant joy, a gargling sound that always made his Mama giggle. He felt a surge of exhilaration, a thrill of conquest. \"Again!\" he seemed to be saying, though all that came out was a happy gurgle.\\n\\nAnd again he tried, again he pulled himself forward. Each inch was a victory, each wobbly attempt a lesson learned. His little limbs, propelled by a newfound determination, navigated the obstacles of the rug. His eyes, wide and full of wonder, tracked the path ahead.\\n\\nMama watched, her smile widening with every scrabble and stretch. She clapped her hands, cheering him on, \"That's it, Pipkin, you're doing it! Crawl, crawl, crawl!\"\\n\\nPipkin didn't know what \"crawl\" meant, but he understood the feeling. The feeling of movement, of control, of the world opening up before him. He crawled towards a brightly-coloured mobile hanging above his crib. He reached for it, his tiny fingers grasping at the dangling toys.\\n\\nHe didn't quite reach, but he didn't care. He had tasted freedom, the sweet freedom of movement. He had discovered the magic of crawling. And as he lay back on his rug, a happy sigh escaping his lips, Pipkin knew that this was only the beginning of his grand adventures. \\n\\n\\nThe world was his to explore, and he was ready to crawl.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Why did the data scientist quit his job?\\n\\nBecause he couldn't regression to his old ways of making a living and had too many correlations to his previous complaints, but ultimately he just needed a feature upgrade in his life.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            To calculate 22 * 13 mentally, you can use the technique of breaking down the numbers and then multiplying them partially.\\n\\nOne way to do it is:\\n\\n1. Break down the multiplication into easier parts: (20 * 13) + (2 * 13)\\n2. Calculate (20 * 13) = (20 * 10) + (20 * 3) \\n  (20 * 10) = 200 \\n  (20 * 3) = 60 \\n  So (20 * 13) = 200 + 60 = 260 \\n3. Then calculate (2 * 13)\\n  (2 * 10) = 20 \\n  (2 * 3) = 6 \\n  So (2 * 13) = 20 + 6 = 26 \\n4. Add both results: (260 + 26) = 286 \\n\\nTherefore, 22 * 13 = 286.  \n",
       "5  **The Crawling Adventure**\\n\\nIn a cozy little house on a quiet street, a tiny miracle was waiting to unfold. Baby Emma, with her soft, fluffy hair and curious eyes, was about to embark on her most epic adventure yet: learning to crawl.\\n\\nEmma's earliest memories were of being held close by her mom, feeling the warmth and love of her touch. She'd gaze up at her mother's smiling face, watching as she played with her fingers, blew gentle kisses, and whispered sweet nothings. But as the days went by, Emma began to feel an inexplicable itch – a sense that there was more to explore, more to discover.\\n\\nOne morning, as the sun peeked through the windows, casting a warm glow over the room, Emma started to stir. She pushed herself up onto her forearms, her eyes scanning the surroundings as if searching for secrets hidden in the crevices of the furniture. Her mom, watching from a nearby chair, smiled knowingly.\\n\\n\"Today's the day!\" she whispered, setting down her book and creeping toward Emma.\\n\\nAt first, Emma was hesitant, her little hands grasping the soft blanket beneath her. But with each gentle urging from her mom, she began to shift, ever so slightly. Her legs twitched, her torso moved, and – in a moment of pure bliss – she launched herself forward, propelled by pure, unadulterated momentum.\\n\\nThe room seemed to spin as Emma crawled, her tiny hands grasping at the air, her feet wobbling in mid-air. Her mom laughed, her eyes shining with pride, as she carefully positioned herself beside Emma.\\n\\n\"Whoa, you're moving!\" she exclaimed, scooping up Emma in a triumphant hug.\\n\\nWith each try, Emma grew bolder, her confidence growing like a bloom of sunflowers in the spring. She inched closer to the bookshelf, fascinated by the towering stacks of picture books. She crept along the couch, exploring the crevices beneath the cushions. Even the carpet became a magical playground, as Emma discovered the delight of rolling onto her tummy, arms and legs splayed wide.\\n\\nOne afternoon, as the sun began to set, casting long shadows across the room, Emma reached the summit – a pile of colorful toys at the far end of the room. With a squeal of excitement, she launched herself toward the treasure trove, her tiny hands grasping for the soft, wiggly creatures. Her mom cheered her on, snapping photos to capture the moment.\\n\\nFrom that day forward, Emma's world expanded exponentially. Every step, every crawl, and every wobble became a thrilling adventure, a journey of discovery and growth. And as she looked up at her mom, beaming with pride, Emma knew she was no longer just a baby – she was a brave, fearless explorer, ready to take on the world.  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None) # allows wide dataframes to be viewed\n",
    "models = [\"gemma2-9b-it\", \"llama-3.1-8b-instant\"] #can edit this\n",
    "\n",
    "# ANSWER\n",
    "prompts = [\n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\",\n",
    "]\n",
    "\n",
    "results = {'Model Name': [], 'Question': [], 'Answer': []}\n",
    "\n",
    "for model in models:\n",
    "    for prompt in prompts:\n",
    "        results['Model Name'].append(model)\n",
    "        results['Question'].append(prompt)\n",
    "        try:\n",
    "            output = client.chat.completions.create(model = model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "            results['Answer'].append(output.choices[0].message.content.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model}: {e}\")\n",
    "            results['Answer'].append((prompt, \"ERROR\"))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5",
   "metadata": {
    "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5"
   },
   "source": [
    "### Bonus\n",
    "\n",
    "See if you can prompt an LLM to perform sentiment analysis (output 'Positive' or 'Negative' only) on a given piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a",
   "metadata": {
    "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "input1 = \"I absolutely loved the way the story unfolded.\"\n",
    "output1 = \"Positive\"\n",
    "\n",
    "input2 = \"The food was cold and completely flavorless.\"\n",
    "output2 = \"Negative\"\n",
    "\n",
    "input3 = \"She handled the situation with grace and professionalism.\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are amazing at sentiment analysis. Give the sentiment of the next sentence as the examples show.\"},\n",
    "        {\"role\": \"user\", \"content\": input1},\n",
    "        {\"role\": \"assistant\", \"content\": output1},\n",
    "        {\"role\": \"user\", \"content\": input2},\n",
    "        {\"role\": \"assistant\", \"content\": output2},\n",
    "        {\"role\": \"user\", \"content\": input3},\n",
    "    ]\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d",
   "metadata": {
    "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279421d-cbca-4003-941e-c2d2bc2833a8",
   "metadata": {
    "id": "f279421d-cbca-4003-941e-c2d2bc2833a8"
   },
   "source": [
    "We worked with a few Large Language Models (LLMs) using Groq and experimented with prompting for summarisation, text completion and question-answering tasks.\n",
    "\n",
    "We also explored controlling the randomness (creativity) of output through the temperature setting and tried different types of prompting to achieve desired forms of output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f06d5e-7073-485b-b046-afe839ab844c",
   "metadata": {
    "id": "94f06d5e-7073-485b-b046-afe839ab844c"
   },
   "source": [
    "## References\n",
    "1. [Groq's prompting guide](https://console.groq.com/docs/prompting)\n",
    "2. [Groq's playground](https://console.groq.com/playground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61aab4a-1330-4762-9318-5635c3a97aa7",
   "metadata": {
    "id": "d61aab4a-1330-4762-9318-5635c3a97aa7"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2025 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
