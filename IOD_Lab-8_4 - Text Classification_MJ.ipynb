{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nji1a9ULLtCA"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnsX1AWKLtCE"
   },
   "source": [
    "# Lab 8.4: Text Classification\n",
    "\n",
    "In this lab you will implement different types of feature engineering for text classification:\n",
    "* Count vectors\n",
    "* TF-IDF vectors (word level, n-gram level, character level)\n",
    "* Text/NLP based features\n",
    "* Topic models\n",
    "  \n",
    "The following classification algorithms will be applied to the count and TF-IDF vector features:\n",
    "* Na√Øve Bayes\n",
    "* Logistic Regression\n",
    "* Support Vector Machine\n",
    "* Random Forest\n",
    "* Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pm8PttyLtCI"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:38:33.182995Z",
     "start_time": "2019-06-17T01:38:30.045388Z"
    },
    "id": "EUANiH6zLtCK"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58bUNQA0LtCV"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqU7d_qcLtCX"
   },
   "source": [
    "Sample:\n",
    "\n",
    "    __label__2 Stuning even for the non-gamer: This sound ...\n",
    "    __label__2 The best soundtrack ever to anything.: I'm ...\n",
    "    __label__2 Amazing!: This soundtrack is my favorite m ...\n",
    "    __label__2 Excellent Soundtrack: I truly like this so ...\n",
    "    __label__2 Remember, Pull Your Jaw Off The Floor Afte ...\n",
    "    __label__2 an absolute masterpiece: I am quite sure a ...\n",
    "    __label__1 Buyer beware: This is a self-published boo ...\n",
    "    . . .\n",
    "    \n",
    "There are only two **labels**:\n",
    "- `__label__1`\n",
    "- `__label__2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:38:42.024845Z",
     "start_time": "2019-06-17T01:38:41.896098Z"
    },
    "id": "rwWFJprZLtCZ"
   },
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "\n",
    "df_corpus = pd.read_fwf(\n",
    "    filepath_or_buffer = 'corpus.txt',\n",
    "    colspecs = [(9, 10),   # label: get only the numbers 1 or 2\n",
    "                (11, 9000) # text: makes the it big enough to get to the end of the line\n",
    "               ],\n",
    "    header = 0,\n",
    "    names = ['label', 'text'],\n",
    "    lineterminator = '\\n'\n",
    ")\n",
    "\n",
    "# convert label from [1, 2] to [0, 1]\n",
    "df_corpus['label'] = df_corpus['label'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mILVIHomLtCf"
   },
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:39:24.213192Z",
     "start_time": "2019-06-17T01:39:24.209202Z"
    },
    "id": "G9_8RbOeLtCh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9999 entries, 0 to 9998\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   9999 non-null   int64 \n",
      " 1   text    9999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n",
      "None\n",
      "      label                                               text\n",
      "1508      0  NOT IMPRESSED: Just got the game....NOT IMPRES...\n",
      "3136      0  Problems installing software, weak tech suppor...\n",
      "3045      1  Darkness rented from Amazon.com: Excellent vid...\n",
      "3581      1  Psycohistory: When you foretell the decline of...\n",
      "773       0  Wasted Time: There is absolutely nothing capti...\n",
      "1073      1  a classic: Call me nostalgic, but this Christm...\n",
      "2872      0  Horrible: It isn't very often that I turn off ...\n",
      "6493      1  Needed in every Brio railway layout: This set ...\n",
      "6495      1  Great Item!: We were so excited to get this it...\n",
      "6996      0  Book was not trimmed: The first copy of the bo...\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "print(df_corpus.info())\n",
    "print(df_corpus.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YmYgG2pLtCu"
   },
   "source": [
    "## Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:39:40.103737Z",
     "start_time": "2019-06-17T01:39:40.100739Z"
    },
    "id": "j5vErjWFLtCy"
   },
   "outputs": [],
   "source": [
    "## ANSWER\n",
    "## split the dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_corpus['text'],\n",
    "    df_corpus['label'],\n",
    "    test_size = 0.2,\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nUp6oDOLtC1"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKd9yTnyLtC2"
   },
   "source": [
    "### Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:32.674674Z",
     "start_time": "2019-06-17T01:40:31.098889Z"
    },
    "id": "DU2RqqDjLtC3"
   },
   "outputs": [],
   "source": [
    "# create a count vectorizer object\n",
    "count_vect = CountVectorizer(token_pattern = r'\\w{1,}')\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "count_vect.fit(X_train)\n",
    "\n",
    "# Transform documents to document-term matrix.\n",
    "X_train_count = count_vect.transform(X_train)\n",
    "X_test_count = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJs6al0ILtC5"
   },
   "source": [
    "### TF-IDF Vectors as features\n",
    "- Word level\n",
    "- N-Gram level\n",
    "- Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:36.088730Z",
     "start_time": "2019-06-17T01:40:34.519925Z"
    },
    "id": "myjfdfP_LtC6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, token_pattern='\\\\w{1,}')\n",
      "CPU times: total: 500 ms\n",
      "Wall time: 514 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer = 'word',\n",
    "                             token_pattern = r'\\w{1,}',\n",
    "                             max_features = 5000)\n",
    "print(tfidf_vect)\n",
    "\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf  = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:40:57.505221Z",
     "start_time": "2019-06-17T01:40:49.387393Z"
    },
    "id": "-h16dUaVLtC_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000, ngram_range=(2, 3), token_pattern='\\\\w{1,}')\n",
      "CPU times: total: 2.78 s\n",
      "Wall time: 2.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ngram level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer = 'word',\n",
    "                                   token_pattern = r'\\w{1,}',\n",
    "                                   ngram_range = (2, 3),\n",
    "                                   max_features = 5000)\n",
    "print(tfidf_vect_ngram)\n",
    "\n",
    "tfidf_vect_ngram.fit(X_train)\n",
    "X_train_tfidf_ngram = tfidf_vect_ngram.transform(X_train)\n",
    "X_test_tfidf_ngram  = tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:41:10.209071Z",
     "start_time": "2019-06-17T01:40:59.211484Z"
    },
    "id": "Y7rmIt49LtDC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='char', max_features=5000, ngram_range=(2, 3))\n",
      "CPU times: total: 3.84 s\n",
      "Wall time: 3.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer = 'char',\n",
    "                                         ngram_range = (2, 3),\n",
    "                                         max_features = 5000)\n",
    "print(tfidf_vect_ngram_chars)\n",
    "\n",
    "tfidf_vect_ngram_chars.fit(X_train)\n",
    "X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_train)\n",
    "X_test_tfidf_ngram_chars  = tfidf_vect_ngram_chars.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Pck1cuvLtDH"
   },
   "source": [
    "### Text / NLP based features\n",
    "\n",
    "Create some other features.\n",
    "\n",
    "char_count = Number of Characters in Text\n",
    "\n",
    "word_count = Number of Words in Text\n",
    "\n",
    "word_density = Average Number of Char in Words\n",
    "\n",
    "punctuation_count = Number of Punctuation in Text\n",
    "\n",
    "title_word_count = Number of Words in Title\n",
    "\n",
    "uppercase_word_count = Number of Upperwords in Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:43:52.813132Z",
     "start_time": "2019-06-17T01:43:52.806150Z"
    },
    "id": "4jebGm1gLtDH"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ANSWER\n",
    "\n",
    "df_corpus['char_count'] = df_corpus['text'].apply(len)\n",
    "df_corpus['word_count'] = df_corpus['text'].apply(lambda x: len(x.split()))\n",
    "df_corpus['word_density'] = df_corpus['char_count'] / (df_corpus['word_count'] + 1)\n",
    "df_corpus['punctuation_count'] = df_corpus['text'].apply(lambda x: len(''.join(_ for _ in x if _ in string.punctuation)))\n",
    "df_corpus['title_word_count'] = df_corpus['text'].apply(lambda x: len([w for w in x.split() if w.istitle()]))\n",
    "df_corpus['uppercase_word_count'] = df_corpus['text'].apply(lambda x: len([w for w in x.split() if w.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent!: My 15 year old daughter is using t...</td>\n",
       "      <td>248</td>\n",
       "      <td>41</td>\n",
       "      <td>5.904762</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2840</th>\n",
       "      <td>0</td>\n",
       "      <td>crappy acting: i was really looking forward to...</td>\n",
       "      <td>969</td>\n",
       "      <td>189</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8396</th>\n",
       "      <td>0</td>\n",
       "      <td>Glad to be done with this one: I pride myself ...</td>\n",
       "      <td>565</td>\n",
       "      <td>109</td>\n",
       "      <td>5.136364</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4086</th>\n",
       "      <td>1</td>\n",
       "      <td>A little hard to fold back up but seems great ...</td>\n",
       "      <td>268</td>\n",
       "      <td>52</td>\n",
       "      <td>5.056604</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8989</th>\n",
       "      <td>0</td>\n",
       "      <td>This book is the worst you could use for your ...</td>\n",
       "      <td>453</td>\n",
       "      <td>80</td>\n",
       "      <td>5.592593</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  char_count  \\\n",
       "4382      1  Excellent!: My 15 year old daughter is using t...         248   \n",
       "2840      0  crappy acting: i was really looking forward to...         969   \n",
       "8396      0  Glad to be done with this one: I pride myself ...         565   \n",
       "4086      1  A little hard to fold back up but seems great ...         268   \n",
       "8989      0  This book is the worst you could use for your ...         453   \n",
       "\n",
       "      word_count  word_density  punctuation_count  title_word_count  \\\n",
       "4382          41      5.904762                  8                 5   \n",
       "2840         189      5.100000                  2                 0   \n",
       "8396         109      5.136364                 14                 9   \n",
       "4086          52      5.056604                  8                 4   \n",
       "8989          80      5.592593                 13                 7   \n",
       "\n",
       "      uppercase_word_count  \n",
       "4382                     0  \n",
       "2840                     0  \n",
       "8396                     7  \n",
       "4086                     3  \n",
       "8989                     7  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:44:03.442730Z",
     "start_time": "2019-06-17T01:44:02.298791Z"
    },
    "id": "Z-l2iZcLLtDO"
   },
   "outputs": [],
   "source": [
    "## load spaCy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-9d0G59LtDR"
   },
   "source": [
    "Part of Speech in **SpaCy**\n",
    "\n",
    "    POS   DESCRIPTION               EXAMPLES\n",
    "    ----- ------------------------- ---------------------------------------------\n",
    "    ADJ   adjective                 big, old, green, incomprehensible, first\n",
    "    ADP   adposition                in, to, during\n",
    "    ADV   adverb                    very, tomorrow, down, where, there\n",
    "    AUX   auxiliary                 is, has (done), will (do), should (do)\n",
    "    CONJ  conjunction               and, or, but\n",
    "    CCONJ coordinating conjunction  and, or, but\n",
    "    DET   determiner                a, an, the\n",
    "    INTJ  interjection              psst, ouch, bravo, hello\n",
    "    NOUN  noun                      girl, cat, tree, air, beauty\n",
    "    NUM   numeral                   1, 2017, one, seventy-seven, IV, MMXIV\n",
    "    PART  particle                  's, not,\n",
    "    PRON  pronoun                   I, you, he, she, myself, themselves, somebody\n",
    "    PROPN proper noun               Mary, John, London, NATO, HBO\n",
    "    PUNCT punctuation               ., (, ), ?\n",
    "    SCONJ subordinating conjunction if, while, that\n",
    "    SYM   symbol                    $, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù\n",
    "    VERB  verb                      run, runs, running, eat, ate, eating\n",
    "    X     other                     sfpksdpsxmsa\n",
    "    SPACE space\n",
    "    \n",
    "Find out the number of Adjectives, Adverbs, Nouns, Numerals, Pronouns, Proper Nouns, Verbs.\n",
    "\n",
    "    Hint:\n",
    "    1. Convert text to spacy document\n",
    "    2. Use pos_\n",
    "    3. Use Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:50:15.900377Z",
     "start_time": "2019-06-17T01:50:15.889406Z"
    },
    "id": "NcxmvIOGLtDS"
   },
   "outputs": [],
   "source": [
    "# Initialise some columns for feature's counts\n",
    "df_corpus['adj_count'] = 0\n",
    "df_corpus['adv_count'] = 0\n",
    "df_corpus['noun_count'] = 0\n",
    "df_corpus['num_count'] = 0\n",
    "df_corpus['pron_count'] = 0\n",
    "df_corpus['propn_count'] = 0\n",
    "df_corpus['verb_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:52:39.611809Z",
     "start_time": "2019-06-17T01:52:39.608818Z"
    },
    "id": "1KfFtu1HcPwA"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# for each text\n",
    "for i in range(df_corpus.shape[0]):\n",
    "    # convert into a spaCy document\n",
    "    doc = nlp(df_corpus.iloc[i]['text'])\n",
    "    # initialise feature counters\n",
    "    c = Counter([t.pos_ for t in doc])\n",
    "\n",
    "    df_corpus.at[i, 'adj_count'] = c['ADJ']\n",
    "    df_corpus.at[i, 'adv_count'] = c['ADV']\n",
    "    df_corpus.at[i, 'noun_count'] = c['NOUN']\n",
    "    df_corpus.at[i, 'num_count'] = c['NUM']\n",
    "    df_corpus.at[i, 'pron_count'] = c['PRON']\n",
    "    df_corpus.at[i, 'propn_count'] = c['PROPN']\n",
    "    df_corpus.at[i, 'verb_count'] = c['VERB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T01:59:42.424828Z",
     "start_time": "2019-06-17T01:59:42.390920Z"
    },
    "id": "DW1_LKP2LtDX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>num_count</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>propn_count</th>\n",
       "      <th>verb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8358</th>\n",
       "      <td>235</td>\n",
       "      <td>49</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>537</td>\n",
       "      <td>92</td>\n",
       "      <td>5.774194</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7815</th>\n",
       "      <td>845</td>\n",
       "      <td>179</td>\n",
       "      <td>4.694444</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>601</td>\n",
       "      <td>108</td>\n",
       "      <td>5.513761</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8195</th>\n",
       "      <td>422</td>\n",
       "      <td>78</td>\n",
       "      <td>5.341772</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      char_count  word_count  word_density  punctuation_count  \\\n",
       "8358         235          49      4.700000                  4   \n",
       "1274         537          92      5.774194                 17   \n",
       "7815         845         179      4.694444                 24   \n",
       "3096         601         108      5.513761                 34   \n",
       "8195         422          78      5.341772                 11   \n",
       "\n",
       "      title_word_count  uppercase_word_count  adj_count  adv_count  \\\n",
       "8358                 5                     4          3          3   \n",
       "1274                 7                     3          9          5   \n",
       "7815                20                    12         12         14   \n",
       "3096                20                     6         10          3   \n",
       "8195                13                     3          6          0   \n",
       "\n",
       "      noun_count  num_count  pron_count  propn_count  verb_count  \n",
       "8358           7          0          10            0           6  \n",
       "1274          23          0          12            0          11  \n",
       "7815          27          0          23            1          26  \n",
       "3096          20          2          14            6          10  \n",
       "8195          14          2           8            7           7  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\n",
    "    'char_count', 'word_count', 'word_density',\n",
    "    'punctuation_count', 'title_word_count',\n",
    "    'uppercase_word_count', 'adj_count',\n",
    "    'adv_count', 'noun_count', 'num_count',\n",
    "    'pron_count', 'propn_count', 'verb_count']\n",
    "\n",
    "df_corpus[cols].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQCAUFWYLtDb"
   },
   "source": [
    "### Topic Models as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:27:09.442903Z",
     "start_time": "2019-06-17T02:24:45.531924Z"
    },
    "id": "wg2mAlkRLtDb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 28.1 s\n",
      "Wall time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train a LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components = 20, learning_method = 'online', max_iter = 20)\n",
    "\n",
    "X_topics = lda_model.fit_transform(X_train_count)\n",
    "topic_word = lda_model.components_\n",
    "vocab = count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:28:11.804475Z",
     "start_time": "2019-06-17T02:28:10.978502Z"
    },
    "id": "_8dIDyHjLtDf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Top Words\n",
      "----- --------------------------------------------------------------------------------\n",
      "    0 ear fit jawbone cute jack size hollywood fits stargate episode\n",
      "    1 cap hatebreed circuit severe realy aluminum dancers becuase mentality hardcore\n",
      "    2 battery adapter cave laptop japanese bear ayla bible clan hooked\n",
      "    3 movie the film this dvd is a watch movies was\n",
      "    4 solo anderson steer condescending motorola spoken ra soldier varies sorbo\n",
      "    5 de la y en el que los del es con\n",
      "    6 edition arrived starts program pillow missing bottle table winston scanner\n",
      "    7 keel salt career west mothman calling joe ourselves discovery bums\n",
      "    8 the i and a to it this of is in\n",
      "    9 i my product for not with they use on amazon\n",
      "   10 spanish eric stewart cook manage funk minds patrick dick whale\n",
      "   11 et scooter trapped est manon il des failing bless korean\n",
      "   12 of book the his in story read by and novel\n",
      "   13 emma pool shoot shades tene eyed joanna keeble leonard incompetence\n",
      "   14 episodes metal superficial buddy paris ark clue warrior scratched lifetime\n",
      "   15 violence owner mission dedicated soldiers celiac saturday activities anthology radical\n",
      "   16 charger color 10 skin share twists peace la utterly portrayal\n",
      "   17 insight states henry gay tuscany suggestions restaurant movements soup os\n",
      "   18 l higgins heater ancient gods kate pushed cornwell victorian odysseus\n",
      "   19 cd music album is the of songs s you this\n"
     ]
    }
   ],
   "source": [
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "print('Group Top Words')\n",
    "print('-----', '-'*80)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    top_words = ' '.join(topic_words)\n",
    "    topic_summaries.append(top_words)\n",
    "    print('  %3d %s' % (i, top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtfnK1jeLtDl"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwwoZ6Qp3psC"
   },
   "source": [
    "Run the following cells to train a number of models on the count vector and TF-IDF vector feature sets generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:12.273365Z",
     "start_time": "2019-06-17T02:34:12.263393Z"
    },
    "id": "uwVaWSyTLtDm"
   },
   "outputs": [],
   "source": [
    "## helper function\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "\n",
    "    return accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:14.900001Z",
     "start_time": "2019-06-17T02:34:14.894016Z"
    },
    "id": "f_onpqUkLtDo"
   },
   "outputs": [],
   "source": [
    "# Keep the results in a dataframe\n",
    "results = pd.DataFrame(columns = ['Count Vectors',\n",
    "                                  'WordLevel TF-IDF',\n",
    "                                  'N-Gram Vectors',\n",
    "                                  'CharLevel Vectors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXwLriDpLtDq"
   },
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:34.147043Z",
     "start_time": "2019-06-17T02:34:34.123096Z"
    },
    "id": "ZcU6IKyNLtDs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors    : 0.8520\n",
      "\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 5.67 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy1 = train_model(MultinomialNB(), X_train_count, y_train, X_test_count)\n",
    "print('NB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:36.399812Z",
     "start_time": "2019-06-17T02:34:36.381861Z"
    },
    "id": "zqEG_ByTLtDv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF : 0.8550\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 4.17 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('NB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:39.076000Z",
     "start_time": "2019-06-17T02:34:39.059047Z"
    },
    "id": "uKEEPNi8LtDy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, N-Gram Vectors   : 0.8380\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 3.21 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(MultinomialNB(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('NB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:42.057019Z",
     "start_time": "2019-06-17T02:34:42.009151Z"
    },
    "id": "M9aIibkBLtD0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, CharLevel Vectors: 0.8195\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 15.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(MultinomialNB(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('NB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:46.265712Z",
     "start_time": "2019-06-17T02:34:46.258734Z"
    },
    "id": "kkrodUzCLtD3"
   },
   "outputs": [],
   "source": [
    "results.loc['Na√Øve Bayes'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2oNfajULtD4"
   },
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:50.841687Z",
     "start_time": "2019-06-17T02:34:48.637032Z"
    },
    "id": "OFBhhPZ6LtD4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors    : 0.8530\n",
      "\n",
      "CPU times: total: 8.16 s\n",
      "Wall time: 706 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Count Vectors\n",
    "accuracy1 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 350), X_train_count, y_train, X_test_count)\n",
    "print('LR, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:51.310433Z",
     "start_time": "2019-06-17T02:34:51.214690Z"
    },
    "id": "C89hRhDiLtD6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF : 0.8720\n",
      "\n",
      "CPU times: total: 203 ms\n",
      "Wall time: 14.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('LR, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:51.749258Z",
     "start_time": "2019-06-17T02:34:51.683435Z"
    },
    "id": "MvhV1jC6LtD9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, N-Gram Vectors   : 0.8315\n",
      "\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 10.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('LR, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:52.635899Z",
     "start_time": "2019-06-17T02:34:52.175122Z"
    },
    "id": "XPjIxmtKLtEA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, CharLevel Vectors: 0.8490\n",
      "\n",
      "CPU times: total: 1.19 s\n",
      "Wall time: 101 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('LR, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:53.029844Z",
     "start_time": "2019-06-17T02:34:53.018872Z"
    },
    "id": "ZFK_LWTcLtED"
   },
   "outputs": [],
   "source": [
    "results.loc['Logistic Regression'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1wYto68LtEE"
   },
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:54.237613Z",
     "start_time": "2019-06-17T02:34:53.406835Z"
    },
    "id": "yYGz8he5LtEE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors    : 0.8345\n",
      "\n",
      "CPU times: total: 922 ms\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Count Vectors\n",
    "accuracy1 = train_model(LinearSVC(), X_train_count, y_train, X_test_count)\n",
    "print('SVM, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:54.743263Z",
     "start_time": "2019-06-17T02:34:54.606629Z"
    },
    "id": "wMt26K0cLtEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF : 0.8620\n",
      "\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 38.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(LinearSVC(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('SVM, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:55.220003Z",
     "start_time": "2019-06-17T02:34:55.119256Z"
    },
    "id": "eFt6Y1VvLtEI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors   : 0.8125\n",
      "\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 22.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(LinearSVC(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('SVM, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:56.139528Z",
     "start_time": "2019-06-17T02:34:55.585010Z"
    },
    "id": "iqhsS579LtEL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, CharLevel Vectors: 0.8585\n",
      "\n",
      "CPU times: total: 531 ms\n",
      "Wall time: 548 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support Vector Machine on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(LinearSVC(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('SVM, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:34:56.501558Z",
     "start_time": "2019-06-17T02:34:56.492592Z"
    },
    "id": "go0bcKeILtEN"
   },
   "outputs": [],
   "source": [
    "results.loc['Support Vector Machine'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLGxWK0yLtEO"
   },
   "source": [
    "### Bagging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:20.807157Z",
     "start_time": "2019-06-17T02:34:56.823697Z"
    },
    "id": "HR8aOytWLtEO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors    : 0.8320\n",
      "\n",
      "CPU times: total: 6.34 s\n",
      "Wall time: 6.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Count Vectors\n",
    "accuracy1 = train_model(RandomForestClassifier(n_estimators = 100), X_train_count, y_train, X_test_count)\n",
    "print('RF, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:30.127233Z",
     "start_time": "2019-06-17T02:35:21.198110Z"
    },
    "id": "zXcTCTEGLtET"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF : 0.8390\n",
      "\n",
      "CPU times: total: 4.25 s\n",
      "Wall time: 4.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('RF, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:35:40.162390Z",
     "start_time": "2019-06-17T02:35:30.607944Z"
    },
    "id": "EnvT8qvSLtEW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, N-Gram Vectors   : 0.7835\n",
      "\n",
      "CPU times: total: 4.36 s\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('RF, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:07.640904Z",
     "start_time": "2019-06-17T02:35:40.542371Z"
    },
    "id": "8jt-dTVELtEX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, CharLevel Vectors: 0.7925\n",
      "\n",
      "CPU times: total: 15.4 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Bagging (Random Forest) on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('RF, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:08.104108Z",
     "start_time": "2019-06-17T02:36:08.097127Z"
    },
    "id": "fVKeCH_VLtEZ"
   },
   "outputs": [],
   "source": [
    "results.loc['Random Forest'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyVz4Q6ILtEa"
   },
   "source": [
    "### Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:37.197296Z",
     "start_time": "2019-06-17T02:36:08.451184Z"
    },
    "id": "8wGvHTg-LtEb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, Count Vectors    : 0.7985\n",
      "\n",
      "CPU times: total: 4.28 s\n",
      "Wall time: 4.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Count Vectors\n",
    "accuracy1 = train_model(GradientBoostingClassifier(), X_train_count, y_train, X_test_count)\n",
    "print('GB, Count Vectors    : %.4f\\n' % accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:36:52.852454Z",
     "start_time": "2019-06-17T02:36:37.714920Z"
    },
    "id": "HJNwQf57LtEd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, WordLevel TF-IDF : 0.7950\n",
      "\n",
      "CPU times: total: 9.38 s\n",
      "Wall time: 9.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy2 = train_model(GradientBoostingClassifier(), X_train_tfidf, y_train, X_test_tfidf)\n",
    "print('GB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:37:02.608379Z",
     "start_time": "2019-06-17T02:36:53.252355Z"
    },
    "id": "iyPqLMgkLtEe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, N-Gram Vectors   : 0.7415\n",
      "\n",
      "CPU times: total: 5.55 s\n",
      "Wall time: 5.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "accuracy3 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n",
    "print('GB, N-Gram Vectors   : %.4f\\n' % accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:14.314194Z",
     "start_time": "2019-06-17T02:37:03.039224Z"
    },
    "id": "1KYdyatTLtEg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB, CharLevel Vectors: 0.8030\n",
      "\n",
      "CPU times: total: 1min 25s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy4 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n",
    "print('GB, CharLevel Vectors: %.4f\\n' % accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:14.683222Z",
     "start_time": "2019-06-17T02:39:14.675213Z"
    },
    "id": "AC0hWO59LtEj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.loc['Gradient Boosting'] = {\n",
    "    'Count Vectors': accuracy1,\n",
    "    'WordLevel TF-IDF': accuracy2,\n",
    "    'N-Gram Vectors': accuracy3,\n",
    "    'CharLevel Vectors': accuracy4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:39:15.024279Z",
     "start_time": "2019-06-17T02:39:15.010319Z"
    },
    "id": "b9fY4J7XLtEk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count Vectors</th>\n",
       "      <th>WordLevel TF-IDF</th>\n",
       "      <th>N-Gram Vectors</th>\n",
       "      <th>CharLevel Vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Na√Øve Bayes</th>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.8195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.8530</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.8315</td>\n",
       "      <td>0.8490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>0.8345</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.8320</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.7835</td>\n",
       "      <td>0.7925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.7985</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.7415</td>\n",
       "      <td>0.8030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Count Vectors  WordLevel TF-IDF  N-Gram Vectors  \\\n",
       "Na√Øve Bayes                    0.8520             0.855          0.8380   \n",
       "Logistic Regression            0.8530             0.872          0.8315   \n",
       "Support Vector Machine         0.8345             0.862          0.8125   \n",
       "Random Forest                  0.8320             0.839          0.7835   \n",
       "Gradient Boosting              0.7985             0.795          0.7415   \n",
       "\n",
       "                        CharLevel Vectors  \n",
       "Na√Øve Bayes                        0.8195  \n",
       "Logistic Regression                0.8490  \n",
       "Support Vector Machine             0.8585  \n",
       "Random Forest                      0.7925  \n",
       "Gradient Boosting                  0.8030  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGs7S0Ge3psJ"
   },
   "source": [
    "Which combination of features and model performed the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > ¬© 2025 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mQCAUFWYLtDb",
    "OXwLriDpLtDq",
    "-2oNfajULtD4",
    "q1wYto68LtEE",
    "gLGxWK0yLtEO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
